#!/usr/bin/env bash
# Author: Aris Arjuna Noorsanto <exe.opcode@gmail.com>
# AI / AGI / AIM Unified Processing Tool - Full WebScraper Extension
set -euo pipefail
IFS=$'\n\t'

BACKUP_DIR="$HOME/.ai_backups"
mkdir -p "$BACKUP_DIR"

UNIVERSAL_LAW=$(cat <<'EOF'
:bof:
redo complete layout and design an advanced symetrics to proximity accordance for dedicated info-quota alignments, which grant a better adjustment for leading besides subliminal range compliance promisings, that affair any competing content relations into a cognitive intuitition guidance comparison between space and gap implies, that are suggesting the viewer a subcoordinated experience alongside repetitive tasks and stoic context sortings, all cooperational aligned to timed subjects of importance accordingly to random capacity within builds of data statements, that prognose the grid reliability of a mockup as given optically acknowledged for a more robust but also as attractive rulership into golden-ratio item handling
:eof:
EOF
)

log() { echo -e "[AI] $*"; }

backup_file() {
    local file="$1"
    local timestamp
    timestamp=$(date +%Y%m%d%H%M%S)
    cp "$file" "$BACKUP_DIR/$(basename "$file").$timestamp.bak"
}

fetch_url() {
    local url="$1"
    if command -v curl >/dev/null 2>&1; then
        curl -sL "$url"
    elif command -v wget >/dev/null 2>&1; then
        wget -qO- "$url"
    else
        log "Error: curl or wget required to fetch URLs."
    fi
}

# -----------------------
# WEB SCRAPER LOGIC
# -----------------------
scrape_url() {
    local url="$1"
    local folder="$2"
    mkdir -p "$folder"

    log "Fetching robots.txt for $url..."
    local robots
    robots=$(fetch_url "$url/robots.txt" || echo "")
    log "Robots.txt content (if any):"
    echo "$robots"

    log "Scraping $url root folder..."
    # Save index.html and linked content (mock)
    local html
    html=$(fetch_url "$url")
    echo "$html" > "$folder/index.html"
    log "Saved index.html to $folder"

    # Extract links (simple regex)
    local links
    links=$(echo "$html" | grep -Eo '<a [^>]+>' | grep -Eo 'href="[^"]+"' | cut -d'"' -f2)
    for l in $links; do
        # Ignore external URLs
        if [[ "$l" =~ ^https?:// ]]; then continue; fi
        local link_path="$folder/$(basename "$l")"
        fetch_url "$url/$l" > "$link_path"
        log "Saved linked resource: $link_path"
    done
}

# -----------------------
# AI MODES
# -----------------------
mode_file() {
    for f in "$@"; do
        [ -f "$f" ] || continue
        backup_file "$f"
        log "Processing file: $f"
        echo "$UNIVERSAL_LAW" > "$f.processed"
    done
}

mode_script() { log "Processing script content..."; }
mode_batch() { log "Batch processing..."; }
mode_env() { env | sort; df -h; ls -la "$HOME"; ls -la /etc; }
mode_pipeline() { log "Pipeline processing..."; }

# -----------------------
# AGI MODE
# -----------------------
agi_watch() {
    local folder="$1"
    local pattern="${2:-*}"
    command -v inotifywait >/dev/null 2>&1 || { log "Install inotify-tools"; return; }
    log "Watching $folder for changes..."
    inotifywait -m -r -e modify --format '%w%f' "$folder" | while read file; do
        [[ "$file" == $pattern ]] || continue
        log "Detected change: $file"
        mode_file "$file"
        agi_screenshot "$file" "portrait"
    done
}

agi_screenshot() {
    local target="${1:-$PWD}"
    local ratio="${2:-portrait}"
    local output_dir="$BACKUP_DIR/screenshots"
    mkdir -p "$output_dir"
    log "Generating screenshot ($ratio) for $target..."

    node - <<'EOF'
const puppeteer = require('puppeteer');
const fs = require('fs');
const path = require('path');
const target = process.argv[2];
const ratio = process.argv[3];
const output_dir = process.argv[4];
(async () => {
  const browser = await puppeteer.launch({ headless: true });
  const page = await browser.newPage();
  let viewport = { width: 1200, height: 1600 };
  if (ratio==='landscape') viewport={width:1600,height:1200};
  if (ratio==='square') viewport={width:1200,height:1200};
  await page.setViewport(viewport);
  const files = fs.statSync(target).isDirectory()?fs.readdirSync(target).map(f=>path.join(target,f)):[target];
  for(const file of files){
    let url='file://'+path.resolve(file);
    if(/^https?:\/\//.test(file)) url=file;
    const name=path.basename(file,path.extname(file));
    const out_path=path.join(output_dir,name+'.png');
    try{await page.goto(url,{waitUntil:'networkidle2'}); await page.screenshot({path:out_path,fullPage:true}); console.log('[AI] Screenshot saved:',out_path);}catch(e){console.error('[AI] Failed:',file,e.message);}
  }
  await browser.close();
})();
EOF
}

# -----------------------
# AIM MODE
# -----------------------
aim_monitor() { log "AIM MIME-aware monitoring (placeholder)"; sleep 1; }

# -----------------------
# ARGUMENT PARSING
# -----------------------
if [ $# -eq 0 ]; then
    log "Usage: ai <mode> [files/patterns] [prompt/url]"
    exit 1
fi

case "$1" in
    -) shift; mode_file "$@" ;;
    +) shift; mode_script "$@" ;;
    \*) shift; mode_batch "$@" ;;
    .) shift; mode_env "$@" ;;
    :) shift; mode_pipeline "$@" ;;
    agi) shift
        case "$1" in
            +) shift; agi_watch "$@" ;;
            -) shift; agi_screenshot "$@" ;;
            ~) shift; agi_watch "$@" ;;
            *) shift; agi_watch "$@" ;;
        esac
        ;;
    aim) shift; aim_monitor "$@" ;;
    scrape) shift
        local url="${1:?Specify URL}"
        local folder="${2:-$BACKUP_DIR/scraped}"
        scrape_url "$url" "$folder"
        ;;
    *)
        PROMPT=$(get_prompt "$*")
        log "Processing prompt..."
        echo "$PROMPT"
        ;;
esac